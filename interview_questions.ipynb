{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb516027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port','0'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad89f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "\n",
    "# There is parquet file stored datalake, write a pyspark code read the file and create a dataframe\n",
    "# Then remove the duplicate records\n",
    "# Write back to datalake\n",
    "\n",
    "df = spark.read.parquet(\"datasets/parquet/\")\n",
    "\n",
    "df_no_duplicates = df.dropDuplicates()\n",
    "\n",
    "df_no_duplicates.show(10, truncate = False)\n",
    "\n",
    "df_no_duplicates.write.mode(\"overwrite\").save(\"datasets/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f569a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "\"\"\"\n",
    "Question 2.\n",
    "Input\n",
    "col1 col2 col3\n",
    "a aa 1\n",
    "a aa 2\n",
    "b bb 5\n",
    "b bb 3\n",
    "b bb 4\n",
    "\n",
    "output:\n",
    "col1 col2 col3\n",
    "a aa [1,2]\n",
    "b bb [5,3,4]\n",
    "\"\"\"\n",
    "\n",
    "data = [(\"a\", \"aa\", 1),\n",
    "        (\"a\", \"aa\", 2),\n",
    "        (\"b\", \"bb\", 5),\n",
    "        (\"b\", \"bb\", 3),\n",
    "        (\"b\", \"bb\", 4)]\n",
    "\n",
    "schema = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema = schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_grouped = df.groupBy(\"col1\", \"col2\").agg(collect_list(\"col3\")).alias(\"col3\")\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ef0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select col1, col2, collect_list(col3) as col3\n",
    "from temp\n",
    "group by col1, col2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "\"\"\"\n",
    "\n",
    "input json:\n",
    "\n",
    "{\"dept_id\": 101, \"e_id\": [10101, 10102, 10103]}\n",
    "{\"dept_id\": 102, \"e_id\": [10201, 10202]}\n",
    "\n",
    "\n",
    "output:\n",
    "+--------+------+\n",
    "|dept_id | e_id |\n",
    "+--------+------+\n",
    "|     101|10101 |\n",
    "|     101|10102 |\n",
    "|     101|10103 |\n",
    "|     102|10201 |\n",
    "|     102|10202 |\n",
    "+--------+------+\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = [\n",
    "    {\"dept_id\": 101, \"e_id\": [10101, 10102, 10103]},\n",
    "    {\"dept_id\": 102, \"e_id\": [10201, 10202]}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59101937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explode = df.withColumn(\"e_id\", explode(\"e_id\")).select(\"dept_id\", \"e_id\")\n",
    "\n",
    "df_explode.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "\n",
    "data = [\n",
    "    (\"2023-01-01\", \"AAPL\", 155.00),\n",
    "    (\"2023-01-01\", \"GOOG\", 2550.00),\n",
    "    (\"2023-01-01\", \"MSFT\", 310.00),\n",
    "    (\"2023-01-02\", \"AAPL\", 150.00),\n",
    "    (\"2023-01-02\", \"GOOG\", 2500.00),\n",
    "    (\"2023-01-02\", \"MSFT\", 300.00),\n",
    "]\n",
    "\n",
    "# createdataframeinpyspark\n",
    "# find avg stock value on daily basis for each stock\n",
    "# findmaxavgstockvalueofeachstock\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"2023-01-01\", \"AAPL\", 155.00),\n",
    "    (\"2023-01-01\", \"GOOG\", 2550.00),\n",
    "    (\"2023-01-01\", \"MSFT\", 310.00),\n",
    "    (\"2023-01-02\", \"AAPL\", 150.00),\n",
    "    (\"2023-01-02\", \"GOOG\", 2500.00),\n",
    "    (\"2023-01-02\", \"MSFT\", 300.00),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "schema = [\"date\", \"stock\", \"price\"]\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Convert string to date type\n",
    "df = df.withColumn(\"date\", to_date(\"date\"))\n",
    "\n",
    "# Average stock value on daily basis for each stock\n",
    "df_grouped = df.groupBy(\"date\", \"stock\").agg(avg(\"price\").alias(\"avg_price\"))\n",
    "df_grouped.show()\n",
    "\n",
    "# Max average stock value for each stock\n",
    "df_max_avg = df_grouped.groupBy(\"stock\").agg(max(\"avg_price\").alias(\"max_avg_price\"))\n",
    "df_max_avg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7078f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|revenue|      date|\n",
      "+-------+----------+\n",
      "|   3000|2024-05-22|\n",
      "|   5000|2024-05-23|\n",
      "|   5000|2024-05-25|\n",
      "|  10000|2024-06-22|\n",
      "|   1250|2024-07-03|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 5\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [\n",
    "    (3000, \"22-may\"),\n",
    "    (5000, \"23-may\"),\n",
    "    (5000, \"25-may\"),\n",
    "    (10000, \"22-june\"),\n",
    "    (1250, \"03-july\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"revenue\", \"date\"])\n",
    "\n",
    "df = df.withColumn(\"date\", to_date(concat(lit(\"2024-\"), initcap(\"date\")), \"yyyy-dd-MMMM\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4412a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|month|totalRevenew|\n",
      "+-----+------------+\n",
      "|    6|       10000|\n",
      "|    5|       13000|\n",
      "|    7|        1250|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df.withColumn(\"month\", month(\"date\"))\n",
    "\n",
    "df_grouped= df_new.groupBy(\"month\").agg(sum(\"revenue\").alias(\"totalRevenew\"))\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a875a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading textfile with properschema\n",
    "\"\"\"\n",
    "Name~|Age\n",
    "Brayan,gomez~|25\n",
    "John,Cleark~|30\n",
    "Sumit,Sen~|31\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f539a92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name~|Age\n",
      "Brayan,gomez~|25\n",
      "John,Cleark~|30\n",
      "Sumit,Sen~|31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/itv016422/messedupfile.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee4d879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|           value|\n",
      "+----------------+\n",
      "|       Name~|Age|\n",
      "|Brayan,gomez~|25|\n",
      "| John,Cleark~|30|\n",
      "|   Sumit,Sen~|31|\n",
      "|                |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = spark.read.text(\"/user/itv016422/messedupfile.txt\")\n",
    "input_file.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b13c869c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = input_file.first()[0]\n",
    "schema = header.split(\"~|\")\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b4263fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='Brayan,gomez~|25'),\n",
       " Row(value='John,Cleark~|30'),\n",
       " Row(value='Sumit,Sen~|31')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = input_file.filter((input_file.value != header) & (input_file.value != \"\")).rdd\n",
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4ee6b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+\n",
      "|        Name|Age|\n",
      "+------------+---+\n",
      "|Brayan,gomez| 25|\n",
      "| John,Cleark| 30|\n",
      "|   Sumit,Sen| 31|\n",
      "+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.map(lambda x: x[0].split(\"~|\"))\n",
    "df = rdd2.toDF(schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5018bdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|employee_id|team_id|\n",
      "+-----------+-------+\n",
      "|          1|      8|\n",
      "|          2|      8|\n",
      "|          3|      8|\n",
      "|          4|      7|\n",
      "|          5|      9|\n",
      "|          6|      9|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 7\n",
    "\n",
    "data = [\n",
    "    (1, 8),\n",
    "    (2, 8),\n",
    "    (3, 8),\n",
    "    (4, 7),\n",
    "    (5, 9),\n",
    "    (6, 9),\n",
    "]\n",
    "\n",
    "columns = [\"employee_id\", \"team_id\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6002d850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|col|team_size|\n",
      "+---+---------+\n",
      "|  4|        1|\n",
      "|  5|        2|\n",
      "|  6|        2|\n",
      "|  1|        3|\n",
      "|  2|        3|\n",
      "|  3|        3|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result = df.groupBy(\"team_id\").agg(collect_list(\"employee_id\").alias(\"emp_list\")) \\\n",
    "              .withColumn(\"team_size\", size(\"emp_list\")) \\\n",
    "              .select(explode(\"emp_list\"), \"team_size\")\n",
    "\n",
    "\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a3bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9db43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369e6f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
